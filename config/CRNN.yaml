feature:
  sample_rate: 16000
  gain: -3
  highpass: 10
  mel_spec:
    n_mels: 128
    n_fft: 2048
    hop_size: 256

pooling_time_ratio: 4

batch_size: 12

model:
  cnn:
    activation: glu
    conv_dropout: 0.5
    kernel_size: [3, 3, 3, 3, 3, 3, 3]
    padding: [1, 1, 1, 1, 1, 1, 1]
    stride: [1, 1, 1, 1, 1, 1, 1]
    nb_filters: [16, 32, 64, 128, 128, 128, 128]
    pooling: [[2, 2], [2, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2]]

  rnn:
    n_hidden: 128
    num_layers: 2
    dropout: 0.

  attention: True

trainer_options:
  accum_grad: 1
  grad_clip: 5.0
  log_interval: 250
  train_steps: 30000
  rampup_length: 7500
  consistency_cost: 2.0
  binarization_type: "global_threshold"
  threshold: 0.5
  early_stopping: False
  patience: 20