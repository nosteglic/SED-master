# random seed
seed: "07"

# wandb init
wandb:
  project: 124-concat
  entity: team-417
  resume: True
  id:

data_root: /data2/syx/DCASE2021
#data_root: /data_ssd1/syx/sed_data/DCASE2021

gen_count: 1
resume:

sync_meta: metadata/train/synthetic.tsv
weak_meta: metadata/train/weak.tsv
unlabel_meta: metadata/train/unlabel_in_domain.tsv
valid_meta: metadata/validation/validation.tsv
valid_audio_dir: audio/validation/validation

max_len_seconds: 10
pooling_time_ratio: 8 # label pooling here because data augmentation may handle label (e.g. time shifting)
# select center frame as a pooled label

feature:
  sample_rate: 16000
  gain: -3
  highpass: 10
  mel_spec:
    n_mels: 64
    n_fft: 1024
    hop_size: 323

norm_mode: gcmvn
# gcmvn: (data - mean) / std
# cmvn: (data - mean(axis=0)) / std(axis=0)
# cmn: data - mean(axis=0)
# min_max: data -= ref_level_db
#          np.clip((data - min_level_db) / -min_level_db, 0, 1)
apply_prob: 0.5

# data augmentation
data_aug:
  semi_supervised_training: True # data, label -> data, data（深拷贝）, label
  time_shift:
    apply: True
    params:
      mean: 0
      std: 90
  frequency_shift:
    apply: False
    params:
      mean: 0
      std: 3
  frequency_mask:
    apply: False
    params:
      num_masks: 1
      mask_param: 100
  add_noise:
    apply: False
    params:
      mean: 0.0
      std: 0.01
      snr: 30

# training
ngpu: 1
# epoch：所有的训练样本输入到模型中称为一个epoch
# iteration：一批样本输入到模型中，成为一个Iteration
# batchszie：批大小，决定一个epoch有多少个Iteration
# iteration = epoch / batchsize
num_workers: 4 # 是否多进程读取数据

# ====== Model architecture setting ======
model:
# CNN setting
  cnn:
    activation: "Relu"
    conv_dropout: 0.1
    kernel_size: [3, 3, 3, 3, 3, 3, 3]
    padding: [1, 1, 1, 1, 1, 1, 1]
    stride: [1, 1, 1, 1, 1, 1, 1]
    nb_filters: [16, 32, 64, 128, 128, 128, 128]
    pooling: [[2, 2], [2, 2], [2, 2], [1, 2], [1, 2], [1, 2], [1, 1]]

  # Transformer/Conformer setting
  encoder_type: Conformer
  encoder:
    adim: 144
    aheads: 4
    dropout_rate: 0.1
    elayers: 3
    eunits: 576
    kernel_size: 7

  pooling: token
# ========================================


trainer_options:
  accum_grad: 1
  grad_clip: 5.0
  log_interval: 250
  train_steps: 30000
  rampup_length: 6000
  consistency_cost: 2.0
  use_mixup: True
  binarization_type: "global_threshold"
  threshold: 0.5
  early_stopping: False
  patience: 20

pretrained:

optimizer: "RAdam"
optimizer_params:
  lr: 0.001
  betas: [0.9, 0.98]
  eps: 0.000000001
  weight_decay: 0.000001

scheduler: "StepLR"
scheduler_params:
  step_size: 10000
  gamma: 0.1